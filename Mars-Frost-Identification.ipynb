{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFRhl_z0oyRB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as op\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "import warnings\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications import EfficientNetB0, ResNet50, VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5TqbhjrEpU-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ11ENI_oyRH"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO,\n",
        "                    datefmt='%H:%M:%S',\n",
        "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
        "\n",
        "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
        "SEED = 17\n",
        "\n",
        "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
        "data_head_dir = Path('./data')\n",
        "\n",
        "# Find all subframe directories\n",
        "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
        "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbjMKM0noyRI"
      },
      "outputs": [],
      "source": [
        "# Load train/val/test subframe IDs\n",
        "def load_text_ids(file_path):\n",
        "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "# Load the subframe names for the three data subsets\n",
        "train_ids = load_text_ids('./train_source_images.txt')\n",
        "validate_ids = load_text_ids('./val_source_images.txt')\n",
        "test_ids = load_text_ids('./test_source_images.txt')\n",
        "\n",
        "# Generate a list containing the dataset split for the matching subdirectory names\n",
        "subdir_splits = []\n",
        "for src_id in src_image_ids:\n",
        "    if src_id in train_ids:\n",
        "        subdir_splits.append('train')\n",
        "    elif src_id in validate_ids:\n",
        "        subdir_splits.append('validate')\n",
        "    elif(src_id in test_ids):\n",
        "        subdir_splits.append('test')\n",
        "    else:\n",
        "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
        "        subdir_splits.append(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHF5xOXoyRI"
      },
      "source": [
        "# Loading and pre processing the data\n",
        "### Note that there are multiple ways to preprocess and load your data in order to train your model in tensorflow. We have provided one way to do it in the following cell. Feel free to use your own method and get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDcDviE4oyRN",
        "outputId": "a9b50b99-bb2a-489f-811b-ca4b5b36a224"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-10 22:30:33.947248: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
            "2023-12-10 22:30:33.947274: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2023-12-10 22:30:33.947284: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "2023-12-10 22:30:33.947323: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-12-10 22:30:33.947341: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "def load_and_preprocess(img_loc, label):\n",
        "    def _inner_function(img_loc, label):\n",
        "        img_loc_str = img_loc.numpy().decode('utf-8')\n",
        "        img = Image.open(img_loc_str).convert('RGB')\n",
        "        img = np.array(img)\n",
        "        img = tf.image.resize(img, [299, 299])\n",
        "        img = img / 255.0\n",
        "        label = 1 if label.numpy().decode('utf-8') == 'frost' else 0\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int64])\n",
        "    X.set_shape([299, 299, 3])\n",
        "    y.set_shape([])  # Scalar label\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def load_subdir_data(dir_path, image_size, seed=None):\n",
        "\n",
        "    \"\"\"Helper to create a TF dataset from each image subdirectory\"\"\"\n",
        "\n",
        "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
        "    tile_dir = dir_path / Path('tiles')\n",
        "    label_dir = dir_path /Path('labels')\n",
        "\n",
        "    loc_list = []\n",
        "\n",
        "    for folder in os.listdir(tile_dir):\n",
        "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
        "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
        "                if file.endswith(\".png\"):\n",
        "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
        "\n",
        "    return loc_list\n",
        "\n",
        "def create_dataset(data):\n",
        "    random.shuffle(data)\n",
        "    img_list, label_list = zip(*data)\n",
        "    img_list_t = tf.convert_to_tensor(img_list)\n",
        "    lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "    tf_data = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "    tf_data = tf_data.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    tf_data = tf_data.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "    return tf_data\n",
        "\n",
        "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
        "tf_dataset_train, tf_dataset_test, tf_dataset_val = [], [], []\n",
        "\n",
        "# Update the batch and buffer size as per your model requirements\n",
        "buffer_size = 64\n",
        "batch_size = 32\n",
        "\n",
        "for subdir, split in zip(subdirs, subdir_splits):\n",
        "    full_path = data_head_dir / subdir\n",
        "    if split=='validate':\n",
        "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='train':\n",
        "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='test':\n",
        "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "\n",
        "tf_dataset_train = create_dataset(tf_data_train)\n",
        "tf_dataset_val = create_dataset(tf_data_val)\n",
        "tf_dataset_test = create_dataset(tf_data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigKU2EJoyRP"
      },
      "source": [
        "### (c) Training CNN + MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WeMAUXgoyRP"
      },
      "outputs": [],
      "source": [
        "def image_augmentation(img, label):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_flip_up_down(img)\n",
        "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
        "    img = tf.image.random_hue(img, max_delta=0.1)\n",
        "    return img, label\n",
        "\n",
        "def build_model():\n",
        "    # Build the model\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(299, 299, 3), kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense layer with dropout\n",
        "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def plot(history):\n",
        "    # Plot training and validation errors vs. epochs\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "def get_predictions(dataset):\n",
        "    y_pred = model.predict(dataset)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true = tf.concat([y for x, y in dataset], axis=0)\n",
        "    return y_true, y_pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VQJQ2uvoyRQ"
      },
      "outputs": [],
      "source": [
        "augmented_dataset_train = tf_dataset_train.map(image_augmentation, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "augmented_dataset_train = augmented_dataset_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TJxS48OQoyRR",
        "outputId": "190b33e3-f10c-4e6d-d628-9cfa9aabb0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-10 22:30:34.522031: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [29679]\n",
            "\t [[{{node Placeholder/_1}}]]\n",
            "2023-12-10 22:30:34.522233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [29679]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-12-10 22:30:34.556017: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "2023-12-10 22:30:34.940126: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "928/928 [==============================] - ETA: 0s - loss: 1.6264 - accuracy: 0.7445"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-10 22:43:02.513835: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [11286]\n",
            "\t [[{{node Placeholder/_1}}]]\n",
            "2023-12-10 22:43:02.514261: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [11286]\n",
            "\t [[{{node Placeholder/_0}}]]\n",
            "2023-12-10 22:43:02.937867: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.78752, saving model to cnn.h5\n",
            "928/928 [==============================] - 822s 885ms/step - loss: 1.6264 - accuracy: 0.7445 - val_loss: 0.7875 - val_accuracy: 0.8602\n",
            "Epoch 2/20\n",
            "299/928 [========>.....................] - ETA: 9:05 - loss: 0.7877 - accuracy: 0.8263"
          ]
        }
      ],
      "source": [
        "checkpoint_filepath = 'cnn.h5'\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,  # Saves only the best model based on the monitored quantity\n",
        "    monitor='val_loss',   # Monitors validation loss\n",
        "    mode='min',           # Mode for monitoring ('min' for loss, 'max' for accuracy, etc.)\n",
        "    save_weights_only=False,  # Saves the entire model, not just the weights\n",
        "    verbose=1             # Display updates about the saved model\n",
        ")\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(augmented_dataset_train, epochs=20, validation_data=tf_dataset_val,callbacks=[early_stopping, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wfQPtgDoyRS"
      },
      "outputs": [],
      "source": [
        "plot(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3LgMxrqoyRS"
      },
      "outputs": [],
      "source": [
        "# Get predictions and true classes for train, validation, and test datasets\n",
        "y_true_train, y_pred_train = get_predictions(tf_dataset_train)\n",
        "y_true_val, y_pred_val = get_predictions(tf_dataset_val)\n",
        "y_true_test, y_pred_test = get_predictions(tf_dataset_test)\n",
        "\n",
        "precision_train_cnn, recall_train_cnn, f1_train_cnn, _ = precision_recall_fscore_support(y_true_train, y_pred_train, average='weighted')\n",
        "precision_val_cnn, recall_val_cnn, f1_val_cnn, _ = precision_recall_fscore_support(y_true_val, y_pred_val, average='weighted')\n",
        "precision_test_cnn, recall_test_cnn, f1_test_cnn, _ = precision_recall_fscore_support(y_true_test, y_pred_test, average='weighted')\n",
        "\n",
        "data = {\n",
        "    'Precision' : [precision_train_cnn, precision_val_cnn, precision_test_cnn],\n",
        "    'Recall' : [recall_train_cnn, recall_val_cnn, recall_test_cnn],\n",
        "    'F1-Score' : [f1_train_cnn, f1_val_cnn, f1_test_cnn]\n",
        "}\n",
        "\n",
        "df_cnn = pd.DataFrame(data, index=['Train', 'Validation', 'Test'])\n",
        "df_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scq7O6X1oyRT"
      },
      "source": [
        "### (d) Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDtKdOMroyRT"
      },
      "outputs": [],
      "source": [
        "def transfer_learning(base_model):\n",
        "    # Freeze all layers of the base model\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Create the new model with the pre-trained base model\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz3p1-dYoyRT"
      },
      "source": [
        "## EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH1dxKsEoyRT"
      },
      "outputs": [],
      "source": [
        "# Create transfer models\n",
        "efficient_net = EfficientNetB0(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
        "model_checkpoint = ModelCheckpoint(filepath='enb0.h5'.format(model_name),save_best_only=True, monitor='val_loss',\n",
        "      mode='min', save_weights_only=False, verbose=1)\n",
        "efficient_net_model = transfer_learning(efficient_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMrlK4pwoyRT"
      },
      "outputs": [],
      "source": [
        "efficient_net_history = efficient_net_model.fit(\n",
        "    augmented_dataset_train,\n",
        "    epochs=20,\n",
        "    validation_data=tf_dataset_val,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stopping, model_checkpoiny]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R6fOpW2oyRU"
      },
      "outputs": [],
      "source": [
        "plot(efficient_net_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBvy3LUWoyRU"
      },
      "outputs": [],
      "source": [
        "# Get predictions and true classes for train, validation, and test datasets\n",
        "y_true_train, y_pred_train = get_predictions(tf_dataset_train)\n",
        "y_true_val, y_pred_val = get_predictions(tf_dataset_val)\n",
        "y_true_test, y_pred_test = get_predictions(tf_dataset_test)\n",
        "\n",
        "precision_train_en, recall_train_en, f1_train_en, _ = precision_recall_fscore_support(y_true_train, y_pred_train, average='weighted')\n",
        "precision_val_en, recall_val_en, f1_val_en, _ = precision_recall_fscore_support(y_true_val, y_pred_val, average='weighted')\n",
        "precision_test_en, recall_test_en, f1_test_en, _ = precision_recall_fscore_support(y_true_test, y_pred_test, average='weighted')\n",
        "\n",
        "data = {\n",
        "    'Precision' : [precision_train_en, precision_val_en, precision_test_en],\n",
        "    'Recall' : [recall_train_en, recall_val_en, recall_test_en],\n",
        "    'F1-Score' : [f1_train_en, f1_val_en, f1_test_en],\n",
        "}\n",
        "\n",
        "df_en = pd.DataFrame(data, index=['Train', 'Validation', 'Test'])\n",
        "df_en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTjUAIyUoyRU"
      },
      "source": [
        "## ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0EO6LHtoyRU"
      },
      "outputs": [],
      "source": [
        "resnet50 = ResNet50(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
        "model_checkpoint = ModelCheckpoint(filepath='rn50.h5'.format(model_name),save_best_only=True, monitor='val_loss',\n",
        "      mode='min', save_weights_only=False, verbose=1)\n",
        "resnet50_model = transfer_learning(resnet50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og2diRrkoyRU"
      },
      "outputs": [],
      "source": [
        "resnet50_history = resnet50_model.fit(\n",
        "    tf_dataset_train_augmented,\n",
        "    epochs=20,\n",
        "    validation_data=tf_dataset_val,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8SvbwQboyRU"
      },
      "outputs": [],
      "source": [
        "plot(resnet50_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqGXu5aNoyRV"
      },
      "outputs": [],
      "source": [
        "# Get predictions and true classes for train, validation, and test datasets\n",
        "y_true_train, y_pred_train = get_predictions(tf_dataset_train)\n",
        "y_true_val, y_pred_val = get_predictions(tf_dataset_val)\n",
        "y_true_test, y_pred_test = get_predictions(tf_dataset_test)\n",
        "\n",
        "precision_train_rn, recall_train_rn, f1_train_rn, _ = precision_recall_fscore_support(y_true_train, y_pred_train, average='weighted')\n",
        "precision_val_rn, recall_val_rn, f1_val_rn, _ = precision_recall_fscore_support(y_true_val, y_pred_val, average='weighted')\n",
        "precision_test_rn, recall_test_rn, f1_test_rn, _ = precision_recall_fscore_support(y_true_test, y_pred_test, average='weighted')\n",
        "\n",
        "data = {\n",
        "    'Precision' : [precision_train_rn, precision_val_rn, precision_test_rn],\n",
        "    'Recall' : [recall_train_rn, recall_val_rn, recall_test_rn],\n",
        "    'F1-Score' : [f1_train_rn, f1_val_rn, f1_test_rn]\n",
        "}\n",
        "\n",
        "df_rn = pd.DataFrame(data, index=['Train', 'Validation', 'Test'])\n",
        "df_rn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFiuKz0VoyRV"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gQL6kkxoyRV"
      },
      "outputs": [],
      "source": [
        "vgg16 = VGG16(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n",
        "model_checkpoint = ModelCheckpoint(filepath='vgg16.h5'.format(model_name),save_best_only=True, monitor='val_loss',\n",
        "      mode='min', save_weights_only=False, verbose=1)\n",
        "vgg16_model = transfer_learning(vgg16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdjWBcNYoyRV"
      },
      "outputs": [],
      "source": [
        "vgg16_history = vgg16_model.fit(\n",
        "    augmented_dataset_train,\n",
        "    epochs=20,\n",
        "    validation_data=tf_dataset_val,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySHuCJxgoyRV"
      },
      "outputs": [],
      "source": [
        "plot_history(vgg16_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCySKoktoyRV"
      },
      "outputs": [],
      "source": [
        "# Get predictions and true classes for train, validation, and test datasets\n",
        "y_true_train, y_pred_train = get_predictions(tf_dataset_train)\n",
        "y_true_val, y_pred_val = get_predictions(tf_dataset_val)\n",
        "y_true_test, y_pred_test = get_predictions(tf_dataset_test)\n",
        "\n",
        "precision_train_vgg, recall_train_vgg, f1_train_vgg, _ = precision_recall_fscore_support(y_true_train, y_pred_train, average='weighted')\n",
        "precision_val_vgg, recall_val_vgg, f1_val_vgg, _ = precision_recall_fscore_support(y_true_val, y_pred_val, average='weighted')\n",
        "precision_test_vgg, recall_test_vgg, f1_test_vgg, _ = precision_recall_fscore_support(y_true_test, y_pred_test, average='weighted')\n",
        "\n",
        "data = {\n",
        "    'Precision' : [precision_train_vgg, precision_val_vgg, precision_test_vgg],\n",
        "    'Recall' : [recall_train_vgg, recall_val_vgg, recall_test_vgg],\n",
        "    'F1-Score' : [f1_train_vgg, f1_val_vgg, f1_test_vgg]\n",
        "}\n",
        "\n",
        "df_vgg = pd.DataFrame(data, index=['Train', 'Validation', 'Test'])\n",
        "df_vgg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKRTLWlOoyRW"
      },
      "source": [
        "## Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_1KJUROoyRW"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    'Model Name' : ['CNN + MLP', 'EfficientNetB0', 'ResNet50', 'VGG16'],\n",
        "    'Precision Train' : [precision_train_cnn, precision_train_en, precision_val_rn, precision_vgg],\n",
        "    'Precision Val' : [precision_val_cnn, precision_val_en, precision_val_rn, precision_val_vgg],\n",
        "    'Precision Test': [precision_test_cnn, precision_test_en, precision_test_rn, precision_test_vgg],\n",
        "    'Recall Train' : [recall_train_cnn, recall_train_en, recall_train_rn, recall_train_vgg],\n",
        "    'Recall Val' : [recall_val_cnn, recall_val_en, recall_val_rn, recall_val_vgg],\n",
        "    'Recall Test' : [recall_test_cnn, recall_test_en, recall_test_rn, recall_test_vgg],\n",
        "    'F1-Score Train' : [f1_train_cnn, f1_train_en, f1_train_rn, f1_train_vgg],\n",
        "    'F1-Score Val' : [f1_val_cnn, f1_val_en, f1_val_rn, f1_val_vgg],\n",
        "    'F1-Score Test' : [f1_test_cnn, f1_test_en, f1_test_rn, f1_test_vgg]\n",
        "}\n",
        "\n",
        "compare_df = pd.DataFrame(data)\n",
        "compare_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}